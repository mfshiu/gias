import os
import json
import time
from dotenv import load_dotenv
from neo4j import GraphDatabase
from openai import OpenAI

# ==========================================
# 1. è¨­å®šèˆ‡åˆå§‹åŒ–
# ==========================================
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7689")
NEO4J_AUTH = None # æ ¹æ“šä½ çš„è¨­å®šï¼ŒNo Auth Mode

if not OPENAI_API_KEY:
    print("âš ï¸ Warning: No API Key found.")

client = OpenAI(api_key=OPENAI_API_KEY)
driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)

MODEL_ID = "gpt-4o-mini"
EMBEDDING_MODEL = "text-embedding-3-small"


# ==========================================
# 2. è¼”åŠ©å‡½æ•¸ (Embedding & Decomposition)
# ==========================================
def get_embedding(text):
    """å°‡æ–‡å­—è½‰ç‚º 1536 ç¶­å‘é‡"""
    text = text.replace("\n", " ")
    return client.embeddings.create(input=[text], model=EMBEDDING_MODEL).data[0].embedding


def llm_query_decomposer(user_query):
    """Step 1: å°‡è¤‡é›œèªå¥æ‹†è§£ç‚ºå–®ä¸€æ„åœ–"""
    prompt = f"""
    You are the GIAS Command Parser.
    Split the user query into a list of independent sub-commands.
    Remove polite words. Keep context.
    Output ONLY a valid JSON list of strings.
    
    User Query: "{user_query}"
    """
    try:
        response = client.chat.completions.create(
            model=MODEL_ID,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        content = response.choices[0].message.content.strip()
        if content.startswith("```"): 
            content = content.replace("```json", "").replace("```", "")
        return json.loads(content)
    except Exception:
        return [user_query]

# ==========================================
# 3. æ ¸å¿ƒå‡½æ•¸ï¼šå‘é‡æª¢ç´¢ (Vector Search)
# ==========================================
def find_action_by_vector(tx, user_sub_command):
    """
    Step 2: åˆ©ç”¨å‘é‡ç›¸ä¼¼åº¦åœ¨ KG ä¸­å°‹æ‰¾æœ€åŒ¹é…çš„ Action
    """
    # 1. å°‡ä½¿ç”¨è€…çš„è‡ªç„¶èªè¨€æŒ‡ä»¤è½‰ç‚ºå‘é‡
    command_vector = get_embedding(user_sub_command)
    
    # 2. Cypher æŸ¥è©¢ï¼šè¨ˆç®— Cosine Similarity
    # æ³¨æ„ï¼šé€™è£¡å‡è¨­ Neo4j 5.x+ æ”¯æ´ vector.similarity.cosine
    # å¦‚æœè³‡æ–™é‡å¤§ï¼Œå»ºè­°å»ºç«‹ Vector Indexï¼Œé€™è£¡æ¼”ç¤ºéæ­·è¨ˆç®— (Brute Force)
    query = """
    MATCH (a:Action)
    WHERE a.vector IS NOT NULL
    WITH a, vector.similarity.cosine(a.vector, $command_vector) AS score
    WHERE score > 0.40  // è¨­å®šä¸€å€‹ç›¸ä¼¼åº¦é–€æª»
    RETURN a.name AS action_name, a.behavior AS behavior, score
    ORDER BY score DESC
    LIMIT 1
    """
    
    result = tx.run(query, command_vector=command_vector).single()
    
    if result:
        return {
            "action": result["action_name"],
            "behavior": result["behavior"],
            "score": result["score"]
        }
    return None

def get_action_slots(tx, action_name):
    """
    ç²å–è©² Action é—œè¯çš„ Slots å®šç¾© (é€é REQUIRES é—œä¿‚)
    """
    query = """
    MATCH (a:Action {name: $action_name})-[r:REQUIRES]->(s:Slot)
    RETURN s.name AS slot_name, r.reason AS reason
    """
    results = tx.run(query, action_name=action_name)
    return [{"name": record["slot_name"], "reason": record["reason"]} for record in results]

# ==========================================
# 4. æ ¸å¿ƒå‡½æ•¸ï¼šæ¨¡æ“¬å‘¼å« (Simulated Execution)
# ==========================================
def extract_parameters(sub_command, action_info, slots_info):
    """
    Step 3: æ ¹æ“šæ‰¾åˆ°çš„ Action å®šç¾©ï¼Œè®“ LLM æå–åƒæ•¸
    """
    if not slots_info:
        return {}

    prompt = f"""
    You are the GIAS Slot Filler.
    
    Target Action: "{action_info['action']}"
    Action Behavior: "{action_info['behavior']}"
    Original Command: "{sub_command}"
    
    Required Slots:
    {json.dumps(slots_info, ensure_ascii=False)}
    
    Task: Extract the values for the required slots from the command.
    Output JSON only: {{ "slot_name": "extracted_value" }}
    If a slot is missing, use null.
    """
    
    response = client.chat.completions.create(
        model=MODEL_ID,
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    content = response.choices[0].message.content.strip()
    if content.startswith("```"): content = content.replace("```json", "").replace("```", "")
    return json.loads(content)

# ==========================================
# 5. ä¸»æµç¨‹ (Main Pipeline)
# ==========================================
def run_gias_pipeline(user_query):
    print(f"\nğŸ”µ [User Input]: {user_query}")
    
    # --- 1. Decomposition ---
    sub_commands = llm_query_decomposer(user_query)
    print(f"ğŸ”¸ [Decomposition]: {sub_commands}")
    
    with driver.session() as session:
        for cmd in sub_commands:
            print(f"\n   ğŸ‘‰ Processing: '{cmd}'")
            
            # --- 2. Vector Search in KG ---
            start_t = time.time()
            match = session.execute_read(find_action_by_vector, cmd)
            
            if match:
                print(f"      âœ… Match Found in KG (Score: {match['score']:.4f})")
                print(f"         Action: {match['action']}")
                print(f"         Desc:   {match['behavior']}")
                
                # --- 3. Context-Aware Slot Filling ---
                # æ’ˆå–è©² Action éœ€è¦ä»€éº¼åƒæ•¸
                slots_schema = session.execute_read(get_action_slots, match['action'])
                
                # è®“ LLM å¡«ç©º
                params = extract_parameters(cmd, match, slots_schema)
                
                # --- 4. Simulate Call ---
                print(f"      ğŸ¤– [Simulating Call]: {match['action']}({params})")
                
            else:
                print("      âŒ No suitable tool found in Knowledge Graph.")
            
            print(f"      (Time: {time.time() - start_t:.2f}s)")

# ==========================================
# 6. åŸ·è¡Œæ¸¬è©¦
# ==========================================
if __name__ == "__main__":
    test_cases = [
        "å¹«æˆ‘æŠŠå®¢å»³çš„å†·æ°£è¨­ç‚º26åº¦", 
        "æé†’æˆ‘æ˜å¤©æ—©ä¸Šä¹é»é–‹æœƒ",
        "æˆ‘æƒ³è½å‘¨æ°å€«çš„æ­Œ", # æ¸¬è©¦æ¨¡ç³Šæ„åœ–
    ]
    
    print("ğŸš€ GIAS Vector-Based Execution Engine Started")
    for q in test_cases:
        run_gias_pipeline(q)
    
    driver.close()
    