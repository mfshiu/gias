import os
import json
import time
from dotenv import load_dotenv
from openai import OpenAI

# ==========================================
# 1. è¨­å®šèˆ‡åˆå§‹åŒ–
# ==========================================
load_dotenv()
# è«‹ç¢ºä¿æ‚¨çš„ .env æª”æ¡ˆä¸­æœ‰ OPENAI_API_KEY
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") 

# å¦‚æžœæ²’æœ‰è®€å–åˆ° Keyï¼Œç°¡å–®é˜²å‘†
if not OPENAI_API_KEY:
    print("âš ï¸ Warning: No API Key found. Please set OPENAI_API_KEY in .env")

client = OpenAI(api_key=OPENAI_API_KEY)
MODEL_ID = "gpt-4o-mini" 

# ==========================================
# 2. æ ¸å¿ƒå‡½æ•¸ï¼šQuery Decomposition
# ==========================================
def llm_query_decomposer(user_query):
    """
    Step 1: Query Decomposition
    Input: è¤‡é›œçš„è‡ªç„¶èªžè¨€å­—ä¸²
    Output: æ‹†è§£å¾Œçš„ List[str]
    """
    prompt = f"""
    You are a command parser for a smart assistant system (GIAS).
    Break down the following user query into a list of independent, executable sub-commands.
    
    Rules:
    1. Split compound commands (e.g., "A and B") into separate items.
    2. Remove polite filler words (e.g., "please", "help me").
    3. Keep context if necessary for the command to make sense.
    4. Output ONLY a valid JSON list of strings.
    
    User Query: "{user_query}"
    """
    
    try:
        response = client.chat.completions.create(
            model=MODEL_ID,
            messages=[
                {"role": "system", "content": "Output only JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0 # è¨­ç‚º 0 ä»¥ç¢ºä¿ç©©å®šæ€§
        )
        
        # è§£æž JSON å­—ä¸²
        content = response.choices[0].message.content.strip()
        # æ¸…ç†å¯èƒ½å‡ºç¾çš„ markdown æ¨™è¨˜ (```json ... ```)
        if content.startswith("```"):
            content = content.replace("```json", "").replace("```", "")
            
        decomposed_list = json.loads(content)
        return decomposed_list

    except Exception as e:
        print(f"Error parsing: {e}")
        # Fallback: å¦‚æžœ LLM å¤±æ•—ï¼Œå›žå‚³åŽŸå§‹å­—ä¸²ä½œç‚ºå–®ä¸€å…ƒç´ çš„ List
        return [user_query]

# ==========================================
# 3. æº–å‚™æ¸¬è©¦è³‡æ–™ (10å€‹æƒ…å¢ƒ)
# ==========================================
test_queries = [
    # 1. ç°¡å–®å¥ (IoT)
    "å¹«æˆ‘æ‰“é–‹å®¢å»³çš„è½åœ°ç‡ˆã€‚",
    # 2. ç°¡å–®å¥ (è³‡è¨Š)
    "ä»Šå¤©å°åŒ—çš„å¤©æ°£å¦‚ä½•ï¼Ÿ",
    # 3. è¤‡åˆå¥ (IoT ä¸¦è¡Œ)
    "æŠŠå†·æ°£é—œæŽ‰ï¼Œé †ä¾¿æŠŠçª—ç°¾æ‹‰é–‹ã€‚",
    # 4. è¤‡åˆå¥ (è·¨é ˜åŸŸï¼šåª’é«” + æŽ§åˆ¶)
    "æ’­æ”¾å‘¨æ°å€«çš„æ­Œï¼Œä¸¦æŠŠéŸ³é‡èª¿å¤§ä¸€é»žã€‚",
    # 5. è¤‡åˆå¥ (æ¢ä»¶éš±å«/å·¨é›†)
    "æˆ‘è¦çœ‹é›»å½±äº†ï¼Œå¹«æˆ‘åˆ‡æ›åˆ°åŠ‡é™¢æ¨¡å¼ã€‚",
    # 6. è¤‡åˆå¥ (å¤šåƒæ•¸å±•é–‹)
    "å¹«æˆ‘è¨­ä¸€å€‹æ˜Žå¤©æ—©ä¸Šä¸ƒé»žå’Œå…«é»žçš„é¬§é˜ã€‚",
    # 7. è¤‡åˆå¥ (å®Œå…¨ç„¡é—œè¯ç•°è³ª)
    "é€™é™„è¿‘æœ‰ä»€éº¼å¥½åƒçš„ï¼Ÿé‚„æœ‰å®¢å»³ç‡ˆæ˜¯ä¸æ˜¯æ²’é—œï¼Ÿ",
    # 8. è¤‡åˆå¥ (è² å‘/ä¿®æ­£/æ™‚åº)
    "åœæ­¢æ’­æ”¾éŸ³æ¨‚ï¼Œæ”¹å ±ä¸€ä¸‹æ–°èžã€‚",
    # 9. è¤‡åˆå¥ (æ¨¡ç³ŠæŒ‡ä»£/é€£çºŒ)
    "æ‰“é–‹é›»è¦–ï¼Œç„¶å¾Œè½‰åˆ° HDMI 1ã€‚",
    # 10. æ¥µç«¯è¤‡åˆ (å¤šé‡ä»»å‹™)
    "é–‹ç‡ˆã€éŽ–ä¸Šå¤§é–€ï¼Œç„¶å¾Œå‘Šè¨´æˆ‘æ˜Žå¤©çš„è¡Œç¨‹ã€‚"
]

# ==========================================
# 4. åŸ·è¡Œæ‰¹æ¬¡é©—è­‰
# ==========================================
print(f"ðŸš€ Starting GIAS Query Decomposition Validation (Model: {MODEL_ID})\n")

for i, query in enumerate(test_queries, 1):
    print(f"--- [Case {i}] ---")
    print(f"ðŸ“¥ Input: {query}")
    
    # å‘¼å« LLM
    start_time = time.time()
    results = llm_query_decomposer(query)
    end_time = time.time()
    
    print(f"ðŸ“¤ Decomposed: {results}")
    print(f"â±ï¸ Time: {end_time - start_time:.2f}s")
    
    # æ¨¡æ“¬ä¸‹ä¸€æ­¥ (Step 2 Hybrid Search)
    print("âš™ï¸  Next Steps:")
    for sub_query in results:
        print(f"   --> Parallel Search (Vector + KG) for: '{sub_query}'")
    print("\n")

print("âœ… Validation Complete.")