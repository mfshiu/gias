import os
import json
import time
from dotenv import load_dotenv
from openai import OpenAI

# ==========================================
# 1. è¨­å®šèˆ‡åˆå§‹åŒ– (Configuration)
# ==========================================
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise ValueError("éŒ¯èª¤ï¼šæœªåµæ¸¬åˆ°ç’°å¢ƒè®Šæ•¸ 'OPENAI_API_KEY'ã€‚")

client = OpenAI(api_key=OPENAI_API_KEY)
MODEL_ID = "gpt-4o-mini"
MODEL_ID = "gpt-4o"

# ã€å„ªåŒ–é»ã€‘å®šç¾©å¸¶æœ‰èªªæ˜çš„åŸå­æ„åœ–
# ä½¿ç”¨å­—å…¸æ ¼å¼ï¼Œæ–¹ä¾¿ LLM ç†è§£æ¯å€‹å·¥å…·çš„ç‰©ç†/è³‡è¨Šæ„ç¾©
KNOWN_ATOMIC_INTENTS = {
    "Move_To(Location)": "Robot moves to a specific physical coordinate or named room (e.g., 'Room 301', 'Entrance').",
    "Turn(Direction)": "Robot rotates its body or head to a specific angle or orientation (e.g., '90_Degrees_Left', 'Towards_Guest').",
    "Query_DB(Key)": "Retrieves data from the central database, such as guest names, preferences, or schedule details.",
    "IoT_Switch(Device_ID, State)": "Controls hardware devices like lights, AC, projectors, or electronic locks. State can be 'On', 'Off', or values like '24C'.",
    "Say(Text)": "Outputs synthesized speech to interact with humans. Used for greeting, status reporting, or giving instructions."
}

# ==========================================
# 2. æ ¸å¿ƒ Prompt èˆ‡ API å‘¼å«
# ==========================================

def build_prompt(current_intent):
    # å°‡å­—å…¸è½‰æ›ç‚ºæ˜“è®€çš„æ¸…å–®å­—ä¸²
    tools_description = "\n".join([f"- {k}: {v}" for k, v in KNOWN_ATOMIC_INTENTS.items()])
    
    return f"""
    You are the "GIAS Intent Decomposition Engine". 
    Break down the User Intent into immediate sub-intents (one level deep only).
    
    ### Available Atomic Intents (Pre-defined Tools)
    {tools_description}
    
    ### Input Context
    - **Current Intent to Decompose**: "{current_intent}"
    
    ### Rules
    1. **Semantic Matching**: Match the sub-intent to a "Pre-defined Tool" if its function aligns with the tool's description.
    2. **Decomposition**: If the intent is too complex for one tool, break it into logical sub-steps.
    3. **Atomic Labeling**:
       - Match a Pre-defined Tool -> `is_atomic: true`, `atomic_source: "pre_defined"`.
       - Specific action NOT covered by tools -> `is_atomic: true`, `atomic_source: "new_generated"`.
       - Needs further breakdown -> `is_atomic: false`, `atomic_source: null`.
    4. **Output Format**: Return ONLY valid JSON.
    
    ### JSON Structure
    {{
      "parent_intent": "string",
      "sub_intents": [
        {{
          "id": "string",
          "content": "string (The executable intent, e.g., 'Move_To(Kitchen)')",
          "is_atomic": boolean,
          "atomic_source": "pre_defined" | "new_generated" | null
        }}
      ],
      "relationships": [
        {{ "type": "Sequence" | "Parallel", "from_id": "string", "to_id": "string" }}
      ]
    }}
    """

def call_llm_decompose(intent):
    try:
        prompt = build_prompt(intent)
        response = client.chat.completions.create(
            model=MODEL_ID,
            messages=[
                {"role": "system", "content": "You are a specialized agent for HTN (Hierarchical Task Network) planning. Output structured JSON for intent decomposition."},
                {"role": "user", "content": prompt}
            ],
            response_format={ "type": "json_object" }
        )
        return json.loads(response.choices[0].message.content.strip())
    except Exception as e:
        print(f"[Error] OpenAI API å‘¼å«å¤±æ•—: {e}", flush=True)
        return None

# ==========================================
# 3. éè¿´é‚è¼¯ (Hierarchical Task Decomposition)
# ==========================================

def recursive_planner(intent, depth=0, max_depth=8):
    indent = "    " * depth
    prefix = "â””â”€â”€ " if depth > 0 else "[ROOT] "
    print(f"{indent}{prefix}è™•ç†æ„åœ–: {intent}", flush=True)
    
    if depth >= max_depth:
        print(f"{indent}    [!] é”åˆ°æœ€å¤§æ·±åº¦ï¼Œåœæ­¢æ‹†è§£ã€‚", flush=True)
        return

    result_json = call_llm_decompose(intent)
    if not result_json:
        return

    sub_intents = result_json.get("sub_intents", [])
    
    for sub in sub_intents:
        content = sub['content']
        is_atomic = sub.get('is_atomic', False)
        source = sub.get('atomic_source')

        if is_atomic:
            marker = "ğŸŸ¢ [EXEC]" if source == "pre_defined" else "ğŸ”´ [NEW]"
            print(f"{indent}    {marker} {content} (Type: {source})", flush=True)
        else:
            time.sleep(0.1) # é¿é–‹æ¥µçŸ­æ™‚é–“å…§çš„é«˜é »è«‹æ±‚
            recursive_planner(content, depth + 1, max_depth)

# ==========================================
# 4. åŸ·è¡Œå…¥å£
# ==========================================

if __name__ == "__main__":
    root_intent = "åŸ·è¡Œ VIP è¨ªå®¢æ¥å¾…èˆ‡å±•ç¤ºå»³è‡ªå‹•åŒ–å·¡æª¢"
    root_intent = "æº–å‚™ 301 æœƒè­°å®¤ï¼Œä¸‹åˆå…©é»è¦è·Ÿå®¢æˆ¶é€²è¡Œè¦–è¨Šææ¡ˆã€‚"
    
    print("=== GIAS æ„åœ–æ‹†è§£ç³»çµ±å•Ÿå‹• (OpenAI Mode) ===", flush=True)
    print(f"[ç³»çµ±è³‡è¨Š] ä½¿ç”¨æ¨¡å‹: {MODEL_ID}", flush=True)
    print("-" * 50, flush=True)
    
    start_time = time.time()
    recursive_planner(root_intent, max_depth=8)
    
    print("-" * 50, flush=True)
    print(f"=== æ‹†è§£å®Œæˆï¼Œç¸½è¨ˆç”¨æ™‚: {time.time() - start_time:.2f} ç§’ ===", flush=True)
